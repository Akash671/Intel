{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# MSTAR SAR Image Classifcation: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO version check:\n",
    "You are currently using the latest development version of Intel® Distribution of OpenVINO™ Toolkit. Alternatively, you can open a version of this notebook for the Intel® Distribution of OpenVINO™ Toolkit LTS version by [clicking this link](../../../../openvino-lts/developer-samples/python/mstar-sar-classification-python/mstar_sar_classification_demo.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample showcases the use of the **Intel® Distribution of OpenVINO™** toolkit to optimize and deploy an internally developed ResNET18 model that classifies Synthetic Aperture Radar (SAR) images associated with 10 separate military vehicle classifiers, such as tanks and armored vehicles. The deployed model processes 3,606 Synthetic Aperture Radar (SAR) images across 10 target classifiers in order to benchmark the model’s Frames Per Second and the Seconds Per Frame across Intel’s hardware portfolio available on Dev Cloud.\n",
    "\n",
    "# Overview of how it works\n",
    "At start-up the sample application reads the command line arguments and loads a network and SAR input image to the Inference Engine (IE) plugin. A job is submitted to an edge compute node with a hardware accelerator such as Intel® HD Graphics GPU and Intel® Movidius™ Neural Compute Stick 2.\n",
    "After the inference is completed on all 3600+ images, the number of correct/incorrect images are stored in the /results directory.\n",
    "\n",
    "# Demonstration objectives\n",
    " * Image as input is supported using **OpenCV**\n",
    " * Inference performed on edge hardware (rather than on the development node hosting this Jupyter notebook)\n",
    " * Accurate classification of Synthetic Aperture Radar images\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "This sample requires the following:\n",
    "- All files are present and in the following directory structure:\n",
    "    - **mstar_sar_classification_demo.ipynb** - This Jupyter* Notebook\n",
    "    - **mstar_sar_classification_run_all.py** - Python* code for SAR image classification application\n",
    "    - **/data/reference-sample-data/mstar-sar-classification-python/model/mstar_sar.pb** - TensorFlow frozen graph of pretrained ResNet18 model (provided)\n",
    "    - **/data/reference-sample-data/mstar-sar-classification-python/TEST** - Directory containing test images for SAR model \n",
    "    - **images/HB15087.jpg** - Test image for Jupyter* Notebook demonstration \n",
    "\n",
    "It is recommended that you have already read the following from [Get Started on the Intel® DevCloud for the Edge](https://devcloud.intel.com/edge/home/):\n",
    "- [Overview of the Intel® DevCloud for the Edge](https://devcloud.intel.com/edge/get_started/devcloud/)\n",
    "- [Overview of the Intel® Distribution of OpenVINO™ toolkit](https://devcloud.intel.com/edge/get_started/openvino/)\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>It is assumed that the server this sample is being run on is on the Intel® DevCloud for the Edge which has Jupyter Notebook customizations and all the required libraries already installed.  If you download or copy to a new server, this sample may not run.</i></div>\n",
    "\n",
    "## Set Up\n",
    "\n",
    "### Import dependencies\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results in this notebook. \n",
    "\n",
    "<br><div class=tip><b>Tip: </b>Select a cell and then use **Ctrl+Enter** to run that cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys \n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import logging as log\n",
    "import os.path as ops\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qarpo.demoutils import *\n",
    "from IPython.display import HTML\n",
    "from argparse import ArgumentParser\n",
    "from openvino.inference_engine import IECore\n",
    "from time import time\n",
    "\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "First, let's try running inference on a single image to see how the Intel® Distribution of OpenVINO™ toolkit works.\n",
    "We will be using Intel® Distribution of OpenVINO™ toolkit Inference Engine (IE) to classify military vehicles as seen in SAR imagery.\n",
    "There are five steps involved in this task:\n",
    "\n",
    "1. Create an Intermediate Representation (IR) Model using the Model Optimizer by Intel\n",
    "2. Choose a device and create IEPlugin for the device\n",
    "3. Read the Model's IR using IENetwork\n",
    "4. Load the IENetwork into the Plugin\n",
    "5. Run inference.\n",
    "\n",
    "### Create Intermediate Representation of the Model using Model Optimizer\n",
    "\n",
    "[Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) creates the Intermediate Representation of the model which is the device-agnostic, generic optimization of the model. Caffe*, TensorFlow*, MXNet*, ONNX*, and Kaldi* models are supported by Model Optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=tip><b>Tip:</b> the '!' is a special Jupyter Notebook command that allows you to run shell commands as if you are in a command line. So the above command will work in a terminal (with '!' removed). </div>\n",
    "\n",
    "**Run the cell below to generate an FP32 precision IR of the ResNet18 model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mo_tf.py \\\n",
    "--input_model /data/reference-sample-data/mstar-sar-classification-python/model/mstar_sar.pb \\\n",
    "--data_type FP32 \\\n",
    "-o models/FP32 \\\n",
    "--input_shape \"[1,128,128,1]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=note><i><b>Note: </b>the previous code cell is a single command line input, which spans 5 lines due to the backslash '\\\\', which is a line continuation character in Bash. </i></div>\n",
    "\n",
    "Here, the arguments are:\n",
    "* --input_model : the original model\n",
    "* --data_type : Data type to use. One of {FP32, FP16, half, float}\n",
    "* --input_shape: shape of data input to the model [N, H, W, C]\n",
    "* -o : output directory\n",
    "\n",
    "This script also supports `-h` that will you can get the full list of arguments that the script accepts. \n",
    "\n",
    "With the `-o` option set as above, this command will write the output to the directory `model/FP32`\n",
    "\n",
    "There are two files produced:\n",
    "```\n",
    "models/FP32/mstar_sar.xml\n",
    "models/FP32/mstar_sar.bin\n",
    "```\n",
    "These will be used later in the exercise.\n",
    "\n",
    "**We will also build an FP16 precision IR for the model using the same process.**\n",
    "\n",
    "This will produce two files as well:\n",
    "```\n",
    "models/FP16/mstar_sar.xml\n",
    "models/FP16/mstar_sar.bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mo_tf.py \\\n",
    "--input_model /data/reference-sample-data/mstar-sar-classification-python/model/mstar_sar.pb \\\n",
    "--data_type FP16 \\\n",
    "-o models/FP16 \\\n",
    "--input_shape \"[1,128,128,1]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference Example\n",
    "Initially we will build an example of running inference on the an image of a **ZSU-23-4 tank**. The image on the **left** shows the tank from the side with a standard RGB camera, while the SAR image looks significantly different as shown on the **right** due to being taken from overhead and being captured using SAR image sensors.   \n",
    "\n",
    "<img src=\"images/640px-ZSU-23-4-Camp-Pendleton.jpg\" height=\"213\" width=\"320\" align=\"left\" title=\"By Sgt. Ryan Ward, U.S. Marine Corps, Public Domain, https://commons.wikimedia.org/w/index.php?curid=478813\" />\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/HB15087.jpg\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xml='models/FP32/mstar_sar.xml'\n",
    "device_arg='CPU'\n",
    "input_arg=['images/HB15087.jpg']\n",
    "class_labels=['ZSU_23_4']\n",
    "iterations=10\n",
    "perf_counts=False\n",
    "labels = np.array(['2S1', 'BMP2', 'BRDM_2', 'BTR70', 'BTR_60', 'D7', 'T62', 'T72', 'ZIL131', 'ZSU_23_4'])\n",
    "\n",
    "log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Engine initialization  and load extensions library\n",
    "We initialize the Inference Engine by calling the class **`IECore()`**. For more details on **`IECore`** see the <a href=https://docs.openvinotoolkit.org/latest/ie_python_api/classie__api_1_1IECore.html>IECore Documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read IR\n",
    "We can use the **`IECore`** function **`read_network`** to import the optimized network IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "net = ie.read_network(model=model_xml, weights=model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing input blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_blob = next(iter(net.input_info))\n",
    "out_blob = next(iter(net.outputs))\n",
    "net.batch_size = len(input_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and pre-process input images\n",
    "First let's load the image using OpenCV.\n",
    "We will also have to do some shape manipulation to convert the image to a format that is compatible with our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, c, h, w = net.input_info[input_blob].input_data.shape\n",
    "images = np.ndarray(shape=(n, c, h, w))\n",
    "for i in range(n):\n",
    "    image = cv2.imread(input_arg[i], 0) # Read image as greyscale\n",
    "    if image.shape[:-1] != (h, w):\n",
    "        log.warning(\"Image {} is resized from {} to {}\".format(input_arg[i], image.shape, (h, w)))\n",
    "        image = cv2.resize(image, (w, h))\n",
    "        \n",
    "    # Normalize to keep data between 0 - 1\n",
    "    image = (np.array(image) - 0) / 255.0\n",
    "\n",
    "    # Change data layout from HWC to CHW\n",
    "    image = image.reshape((1, 1, h, w))    \n",
    "    images[i] = image\n",
    "    \n",
    "log.info(\"Batch size is {}\".format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model to the plugin\n",
    "Once we have the Inference Engine and the network, we can load the network into the Inference Engine using **`ie.load_network`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_net = ie.load_network(network=net, device_name=device_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference\n",
    "We can now run inference on the object  **`exec_net`** using the **`infer`** function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_time = []\n",
    "for i in range(iterations):\n",
    "    t0 = time()\n",
    "    result = exec_net.infer(inputs={input_blob: images})\n",
    "    infer_time.append((time()-t0)*1000)\n",
    "\n",
    "result = result[out_blob]\n",
    "    \n",
    "log.info(\"Average running time of one iteration: {} ms\".format(np.average(np.asarray(infer_time))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing output blob\n",
    "The network outputs a tensor of dimension 10 (number of classes), these values represent the probability that the image is a particular class. To make the final classification we find the class with the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the results and get the index of the highest confidence score\n",
    "\n",
    "# Predicted class index.\n",
    "for index, res in enumerate(result):\n",
    "    \n",
    "    class_num = np.argmax(res)\n",
    "    if labels[class_num] == class_labels[index]:\n",
    "        print (\"Result correctly classified as: {}\".format(labels[class_num]))\n",
    "    else:\n",
    "        print (\"Result incorrectly classified as: {}\".format(labels[class_num]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Submission\n",
    "\n",
    "The inference code is already implemented in \n",
    "<a href=\"mstar_sar_classification_run_all.py\">mstar_sar_classification_run_all.py</a>.\n",
    "\n",
    "The Python code takes in command line arguments for images, model etc.\n",
    "\n",
    "**Command line arguments options and how they are interpreted in the application source code**\n",
    "\n",
    "```\n",
    "python3 mstar_sar_classification_run_all.py  -m ${MODELPATH} \\\n",
    "                                             -i ${INPUT_ADDR} \\\n",
    "                                             -o ${OUTPUT_DIR} \\\n",
    "                                             -d ${DEVICE} \\\n",
    "                                             -pc\n",
    "```\n",
    "\n",
    "**The description of the arguments used in the argument parser is the command line executable equivalent.**\n",
    "* -m Location of the model's IR file (.xml + .bin) which has been converted using the **model optimizer**.\n",
    "   There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware\n",
    "* -i Path of the input images \n",
    "* -o Location where the output file with inference needs to be stored. (results/)\n",
    "* -d Type of Hardware Acceleration (CPU, GPU, MYRIAD)\n",
    "* -l Absolute path to extension library file to load to a plugin. (Optional)\n",
    "* -pc Report individual model layer performance counts will be reported to log file in the results/ directory (or as specified with -o argument) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the job file\n",
    "\n",
    "We will run inference on several different edge compute nodes present in the Intel® DevCloud for the Edge. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is a [Bash](https://www.gnu.org/software/bash/) script that serves as a wrapper around the Python* executable of our application that will be executed directly on the edge compute node.  One purpose of the job file is to simplify running an application on different compute nodes by accepting a few arguments and then performing accordingly any necessary steps before and after running the application executable.  \n",
    "\n",
    "For this sample, the job file we will be using is already written for you and appears in the next cell.  The job file will be submitted as if it were run from the command line using the following format:\n",
    "```bash\n",
    "mstar_sar_job.sh <output_directory> <device> <fp_precision> <input_file> \n",
    "```\n",
    "Where the job file input arguments are:\n",
    "- <*output_directory*> - Output directory to use to store output files\n",
    "- <*device*> - Hardware device to use (e.g. CPU, GPU, etc.)\n",
    "- <*fp_precision*> - Which floating point precision inference model to use (FP32 or FP16)\n",
    "- <*input_file*> - Path to input image file(s)\n",
    "\n",
    "Run the following cell to create the `mstar_sar_job.sh` job file.  The [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) line at the top will write the cell contents to the specified job file `mstar_sar_job.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mstar_sar_job.sh\n",
    "\n",
    "# MSTAR SAR job script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "# The output directory is the first argument of the bash script\n",
    "OUTPUT_DIR=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "INPUT_ADDR=$4\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "mkdir -p $1\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 mstar_sar_classification_run_all.py    -m models/$3/mstar_sar.xml  \\\n",
    "                                               -i $4 \\\n",
    "                                               -o $1 \\\n",
    "                                               -d $2 \\\n",
    "                                               -pc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit `mstar_sar_job.sh` to several different types of edge compute nodes simultaneously or just one node at a time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets us name the job so that it is easier to distinguish between them.\n",
    "\n",
    "The `-F` flag is used to pass in arguments to the job script.\n",
    "The [mstar_sar_job.sh](mstar_sar_job.sh) takes in 4 arguments:\n",
    "1. the path to the directory for the performance stats\n",
    "2. targeted device (e.g. CPU, GPU, MYRIAD)\n",
    "3. the floating precision to use for inference\n",
    "4. the path to the input images\n",
    "\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### Job queue submission\n",
    "\n",
    "Each of the 5 cells below will submit a job to different edge compute nodes.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>You may submit all jobs at once or one at a time. After submission, they will go into a queue and run as soon as the requested compute resources become available.</i></div> \n",
    " \n",
    "\n",
    "<br><div class=tip><b>Tip: </b>**Shift+Enter** will run the cell and automatically move you to the next cell. This allows you to use **Shift+Enter** multiple times to quickly run through multiple cells, including markdown cells.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"IMAGES\"] = \"/data/reference-sample-data/mstar-sar-classification-python/TEST\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/content/www/us/en/ark/products/97121/intel-core-i5-7500t-processor-6m-cache-up-to-3-30-ghz.html\">Intel® Core™ i5-7500T processor</a>. The inference workload will run the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core_kaby = !qsub mstar_sar_job.sh -l nodes=1:idc006kbl -F \"results/ CPU FP32 $IMAGES\" -N mstar_sar_core_kaby\n",
    "print(job_id_core_kaby[0])\n",
    "jobid_core_kaby = job_id_core_kaby[0].split('.')[0]\n",
    "#Progress indicators\n",
    "if job_id_core_kaby:\n",
    "    progressIndicator('results/', 'i_progress_'+job_id_core_kaby[0]+'.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Xeon® Gold 6258R CPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Xeon® Gold 6258R Processor](https://ark.intel.com/content/www/us/en/ark/products/199350/intel-xeon-gold-6258r-processor-38-5m-cache-2-70-ghz.html). The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon_cascade_lake = !qsub mstar_sar_job.sh -l nodes=1:idc018 -F \"results/ CPU FP32 $IMAGES\" -N mstar_sar_xeon_cascade_lake \n",
    "print(job_id_xeon_cascade_lake[0]) \n",
    "jobid_xeon_cascade_lake = job_id_xeon_cascade_lake[0].split('.')[0]\n",
    "#Progress indicators\n",
    "if job_id_xeon_cascade_lake:\n",
    "    progressIndicator('results/', 'i_progress_'+job_id_xeon_cascade_lake[0]+'.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Xeon® E3-1268L v5 CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel® \n",
    "    Xeon® Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon_skylake = !qsub mstar_sar_job.sh -l nodes=1:idc007xv5 -F \"results/ CPU FP32 $IMAGES\" -N mstar_sar_xeon_skylake \n",
    "print(job_id_xeon_skylake[0]) \n",
    "jobid_xeon_skylake = job_id_xeon_skylake[0].split('.')[0]\n",
    "#Progress indicators\n",
    "if job_id_xeon_skylake:\n",
    "    progressIndicator('results/', 'i_progress_'+job_id_xeon_skylake[0]+'.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Core i7 CPU and using the onboard Intel® GPU (UHD-620)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://www.aaeon.com/en/p/iot-gateway-node-systems-upx-edge\">UPX-Edge</a> edge node with an <a href=\"https://ark.intel.com/content/www/us/en/ark/products/193554/intel-core-i7-8665ue-processor-8m-cache-up-to-4-40-ghz.html\">Intel® Core i7-8665UE</a>. The inference workload will run on the Intel® UHD Graphics 620 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_corei7_gpu = !qsub mstar_sar_job.sh -l nodes=1:idc014upxa10fx1 -F \"results/ GPU FP32 $IMAGES\" -N mstar_sar_corei7_gpu\n",
    "print(job_id_corei7_gpu[0])\n",
    "jobid_corei7_gpu = job_id_corei7_gpu[0].split('.')[0]\n",
    "#Progress indicators\n",
    "if job_id_corei7_gpu:\n",
    "    progressIndicator('results/', 'i_progress_'+job_id_corei7_gpu[0]+'.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Core i5 CPU and using the onboard Intel® GPU (HD-630)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/content/www/us/en/ark/products/97121/intel-core-i5-7500t-processor-6m-cache-up-to-3-30-ghz.html\">Intel® Core i5-7500T</a>. The inference workload will run on the Intel® HD Graphics 630 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_corei5gpu = !qsub mstar_sar_job.sh -l nodes=1:idc006kbl -F \"results/ GPU FP32 $IMAGES\" -N mstar_sar_corei5gpu\n",
    "print(job_id_corei5gpu[0])\n",
    "jobid_corei5gpu = job_id_corei5gpu[0].split('.')[0]\n",
    "#Progress indicators\n",
    "if job_id_corei5gpu:\n",
    "    progressIndicator('results/', 'i_progress_'+job_id_corei5gpu[0]+'.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job(s)).\n",
    "There should also be an extra job in the queue named `jupyterhub-singleuser`: this job is your current Jupyter* Notebook session which is always running\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "<br><div class=note><i><b>\n",
    "Note: The amount of time spent in the queue depends on the number of users accessing the requested compute nodes. Once the jobs for this sample application begin to run, they should take from 1 to 5 minutes each to complete.\n",
    "</b></i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=danger><b>Wait!: </b>Please wait for the inference jobs to complete before proceeding to the next step to view results.</div>\n",
    "\n",
    "\n",
    "## View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the stdout and stderr streams of each job into files with names of the form\n",
    "\n",
    "`mstar_sar_{type}.o{JobID}`\n",
    "\n",
    "`mstar_sar_{type}.e{JobID}`\n",
    "\n",
    "(here, mstar_sar_{type} corresponds to the `-N` option of qsub).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Performance\n",
    "\n",
    "The running time of each inference task is recorded in `results/stats_job_id_{ARCH}.txt`, where the subdirectory name corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arch_list = [('core_kaby', 'Intel Core\\ni5-7500T\\nCPU'),\n",
    "             ('xeon_cascade_lake', 'Intel Xeon\\nGold\\n 6258R\\nCPU'),\n",
    "             ('xeon_skylake', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('corei7_gpu', ' Intel Core\\ni7-8665UE\\nGPU'),\n",
    "             ('corei5gpu', 'Intel\\nCore\\ni5-7500T\\nGPU')]\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/stats_'+vars()['job_id_'+arch][0]+'.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Time per Image, seconds', 'Inference Engine Processing Time per Image', 'time' )\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telemetry Dashboard\n",
    "\n",
    "Once your submitted jobs are completed, run the cells below to generate links to view telemetry dashboards containing performance metrics for your model and target architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view telemetry dashboard of the last job ran on Intel® Core™ i5-7500T</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + jobid_core_kaby\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view metering dashboard of the last job ran on Intel® Xeon® Gold 6258R CPU</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + jobid_xeon_cascade_lake\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view metering dashboard of the last job ran on Intel® Xeon® E3-1268L CPU</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + jobid_xeon_skylake\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view metering dashboard of the last job ran on Intel® Core i7-8665UE CPU and using the onboard Intel® GPU</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + jobid_corei7_gpu\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view metering dashboard of the last job ran on Intel® Core i5-7500T CPU and using the onboard Intel® GPU</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + jobid_corei5gpu\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- [More Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials) - sample application Jupyter* Notebook tutorials\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "For technical support, please see the [Intel® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2021.1)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
