{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# MICCAI 2017 Robotic Instrument Segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO version check:\n",
    "You are currently using the latest development version of Intel® Distribution of OpenVINO™ Toolkit. Alternatively, you can open a version of this notebook for the Intel® Distribution of OpenVINO™ Toolkit LTS version by [clicking this link](../../../../openvino-lts/developer-samples/python/surgery-segmentation-python/robotic-surgery-segmentation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This sample requires the following:\n",
    "- All files are present and in the following directory structure:\n",
    "    \n",
    "    - **robotic-surgery-segmentation.ipynb** - This Jupyter* Notebook\n",
    "    - **figues/segmentation.gif,TernausNet.png** - Images that appear in this Jupyter* Notebook\n",
    "    - **python/\\*** - Python* code for all aspects of the robotic instrument segmentation application:<br>\n",
    "    -- **figures.py** - Generates visualization for steps of segmentation performed<br>\n",
    "    -- **models.py** - Contains different models as Python* code<br>\n",
    "    -- **pytorch_to_onnx.py** - Converts PyTorch* to ONNX* model<br>\n",
    "    -- **img_to_video.py** - Converts multiple images into a video file<br>\n",
    "    -- **pytorch_infer.py** - Runs inference using PyTorch\\*<br>\n",
    "    -- **segmentation_parts.py** - Robotic instrument segmentation application code<br>\n",
    "    -- **utils.py** - Utility functions<br>\n",
    "    -- **run_py.sh** - Utility shell script to run Python* files from jobs<br>\n",
    "    -- **quantize.py** - Converts OpenVINO FP32 to INT8 model<br>\n",
    "    - **data/short_source.mp4** - Test video\n",
    "    - **data/frame.png,left_framge.png,right_frame.png** - Used for visualization \n",
    "    - **/data/robotic-instrument-segmentation/unet11_\\*_20/model_0.pt** - Trained U-Net model data for different approaches (binary, parts, instruments - \"parts\" will be used)\n",
    "    \n",
    "\n",
    "It is recommended that you have already read the following from [Get Started on the Intel® DevCloud for the Edge](https://devcloud.intel.com/edge/home/):\n",
    "- [Overview of the Intel® DevCloud for the Edge](https://devcloud.intel.com/edge/get_started/devcloud/)\n",
    "- [Overview of the Intel® Distribution of OpenVINO™ toolkit](https://devcloud.intel.com/edge/get_started/openvino/)\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>It is assumed that the server this sample is being run on is on the Intel® DevCloud for the Edge which has Jupyter* Notebook customizations and all the required libraries already installed.  If you download or copy to a new server, this sample may not run.</i></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This sample application demonstrates how a smart video IoT solution may be created using Intel® hardware and software tools to perform robotic instrument segmentation.  This solution performs semantic segmentation to identify the segments of robotic instruments within a video frame.  The identified robotic instruments segments are then highlighted in the output with each segment appearing in a different color.\n",
    "\n",
    "### Key concepts\n",
    "This sample application includes an example for the following:\n",
    "- Application:\n",
    "  - Video and image input is supported using OpenCV\n",
    "  - OpenCV is used to draw bounding boxes around detected objects, labels, and other information\n",
    "  - Visualization of the resulting segmentation in the output\n",
    "- Intel® DevCloud for the Edge:\n",
    "  - Submitting inference as jobs that are performed on different edge compute nodes (rather than on the development node hosting this Jupyter* notebook)\n",
    "  - Monitoring job status\n",
    "  - Viewing results and assessing performance for hardware on different compute nodes\n",
    "- [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/openvino-toolkit):\n",
    "  - Create the necessary Intermediate Representation (IR) files for the inference model using [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)\n",
    "  - Run an inference application on multiple hardware devices using the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application background\n",
    "![Robotic Instrument Challenge](./figures/segmentation.gif)\n",
    "\n",
    "The code in this sample refers to the winning solution by Alexey Shvets, Alexander Rakhlin, Alexandr A. Kalinin, and Vladimir Iglovikov in the [MICCAI 2017 Robotic Instrument Segmentation Challenge](https://endovissub2017-roboticinstrumentsegmentation.grand-challenge.org/). This Jupyter* Notebook has been modified from the original found on [GitHub](https://github.com/ternaus/robot-surgery-segmentation/blob/master/Demo.ipynb) which is made available with an [MIT license](https://github.com/ternaus/robot-surgery-segmentation/blob/master/LICENSE). The data files necessary to run this notebook are included in `/data/robotic-surgery-segmentation`.\n",
    "\n",
    "![TernausNet](./figures/TernausNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robotic instrument segmentation application\n",
    "The robotic instrument segmentation application uses the Intel® Distribution of OpenVINO™ toolkit to perform inference on an input video to locate robotic instruments within each frame.  We will setup, run, and view the results for this application for several different hardware devices (CPU. GPU, etc.) available on the compute nodes within the Intel® DevCloud for the Edge.  To accomplish this, we will be performing the following tasks:\n",
    "\n",
    "1. Use the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) to create the inference model IR files needed to perform inference\n",
    "2. Create the job file used to submit running inference on compute nodes\n",
    "3. Submit jobs for different compute nodes and monitor the job status until complete\n",
    "4. View results and assess performance \n",
    "\n",
    "### How it works\n",
    "At startup the robotic instrument segmentation application configures itself by parsing the command line arguments.  Once configured, the application loads the specified inference model's IR files into the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) and runs inference on the specified input video to identify segments of robotic instruments.  Once identified, each robotic instrument segment is colored to highlight it on the output video.\n",
    "\n",
    "To run the application on the Intel® DevCloud for the Edge, a job is submitted to an edge compute node with a hardware accelerator such as Intel® HD Graphics GPU and Intel® Movidius™ Neural Compute Stick 2.  After inference on the input is completed, the output is stored in the appropriate `results/<architecture>/` directory.  The results are then viewed within this Jupyter* Notebook using the `videoHTML` video playback utility.\n",
    "\n",
    "The application and inference code for this sample is already implemented in the Python* file [`segmentation_parts.py`](./python/segmentation_parts.py) (and other helper `python/*.py` files).\n",
    "\n",
    "The following sections will guide you through configuring and running the robotic instrument segmentation application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "The following sections describe all the necessary configuration to run the robotic instrument segmentation application.\n",
    "#### Command line arguments\n",
    "The application is run from the command line using the following format:\n",
    "```bash\n",
    "python3 python/segmentation_parts.py <arguments...>\n",
    "```\n",
    "The required command line _<arguments...>_ to run the Python* executable [`segmentation_parts.py`](./python/segmentation_parts.py) are:\n",
    "- **-m** - The precision to complete the path to the _.xml_ IR file (created using [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)) for the inference model. The file is found using the path `models/ov/<`*precision*`>/surgical_tools_parts.xml`\n",
    "- **-i** - Path to input video file\n",
    "- **-o** - The path to where the output video file will be stored\n",
    "- **-d** - Device type to use to run inference (CPU, GPU, MYRIAD, CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We begin by importing all the Python* modules that will be used within this Jupyter* Notebook to run and display the results of the robotic instrument segmentation application on the Intel® DevCloud for the Edge:\n",
    "- [cv2](https://opencv.org/) - Python* OpenCV module\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) - Operating system specific module (used for file name parsing)\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) - Time tracking module (used for measuring execution time)\n",
    "- [matplotlib.pyplot](https://matplotlib.org/) - pyplot is used for displaying output images\n",
    "- [sys](https://docs.python.org/3/library/sys.html#module-sys) - System specific parameters and functions\n",
    "- [qarpo.demoutils](https://github.com/ColfaxResearch/qarpo) - Provides utilities for displaying results and managing jobs from within this Jupyter* Notebook\n",
    "- [python.utils](./python/utils.py) - Provides utilities for displaying results within this Jupyter* Notebook\n",
    "\n",
    "Run the following cell to import the Python* dependencies needed.\n",
    "\n",
    "<br><div class=tip><b>Tip: </b>Select a cell and then use **Ctrl+Enter** to run that cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from qarpo.demoutils import *\n",
    "from python.utils import create_script, mask_overlay\n",
    "print('Imported Python modules successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with PyTorch (Optional)\n",
    "Run the following cell to perform inference with the PyTorch model on a compute node with an [Intel Core i5-6500TE](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-) using the code in [pytorch_infer.py](python/pytorch_infer.py). \n",
    "\n",
    "<br><div class=note><i><b>Note: </b>The next cell uses `qsub` to submit a job to be run on a compute node because the login node running this Jupyter* Notebook does not have enough memory to run the task.</i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script\n",
    "job_id_infer = !qsub python/run_py.sh -l nodes=1:idc001skl -F \"python/pytorch_infer.py\" -N pytorch_core\n",
    "if job_id_infer:\n",
    "    print(job_id_infer[0])\n",
    "    progressIndicator('results/', job_id_infer[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=danger><b>Wait!: </b>\n",
    "Please wait for the progress bar and job to complete before proceeding to the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the original input image and the inference results, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read, label, and display original input image\n",
    "image = cv2.imread(\"generated/input.png\")\n",
    "plt.figure(1, figsize=(15, 15))\n",
    "plt.subplot(121)\n",
    "plt.axis('off')\n",
    "plt.title(\"Input Image\")\n",
    "plt.imshow(image)\n",
    "\n",
    "# read, lable, and display result image\n",
    "mask = cv2.imread(\"generated/mask.png\")\n",
    "plt.subplot(122)\n",
    "plt.axis('off')\n",
    "plt.title(\"Segmentation\")\n",
    "plt.imshow(mask_overlay(image, mask));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting PyTorch* to ONNX*\n",
    "\n",
    "The ONNX* models need to be generated from the original PyTorch* models to be used with the Intel® Distribution of OpenVINO™ toolkit.  This is done by running [pytorch_to_onnx.py](python/pytorch_to_onnx.py). \n",
    "\n",
    "<br><div class=note><i><b>Note: </b>The next cell uses `qsub` to submit a job to be run on a compute node because the login node running this Jupyter* Notebook does not have enough memory to run the task.</i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script\n",
    "job_id_onnx = !qsub python/run_py.sh -l nodes=1:idc001skl -F \"python/pytorch_to_onnx.py\" -N pytorch_to_onnx\n",
    "if job_id_onnx:\n",
    "    print(job_id_onnx[0])\n",
    "    progressIndicator('results/', job_id_onnx[0]+'.txt', \"Conversion\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=danger><b>Wait!: </b>\n",
    "Please wait for the progress bar and job to complete before proceeding to the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two ONNX* models that have now be generated are:\n",
    "- **models/onnx/surgical_tools_parts.onnx** - Segmentation model used for identifying robotic instrument segments (\"parts\")\n",
    " - This model will be used to process video and measure performance in later steps \n",
    "- **models/onnx/surgical_tools.onnx** - Segmentation model used for identifying robotic instrument (\"binary\")\n",
    " - This model, along with the `surgical_tools_parts.onnx`, will be used in an extra inference run to show some visualization of the segmentation involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Create the IR files for the inference model\n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into the Intermediate Representation (IR) model files, and the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) that uses the IR model files to run inference on hardware devices.  The IR model files can be created from trained models from popular frameworks (e.g. Caffe\\*, Tensorflow*, etc.). For this sample, the two ONNX* models that will be used were just generated from PyTorch* in the previous step.\n",
    "\n",
    "\n",
    "\n",
    "The `surgical_tools_parts.onnx` and `surgical_tools.onnx` files will need to be optimized using the Model Optimizer to create the necessary IR files.  We will be running the inference model on different hardware devices which have different requirements on the precision of the model (see [Inference Engine Supported Model Formats](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_Supported_Devices.html#supported_model_formats) for details).  For our purposes, we will focus on the use of the most common precision `FP16` to run on all devices that we target.\n",
    "\n",
    "For this model, we will run the Model Optimizer using the format:\n",
    "```bash\n",
    "mo.py \\\n",
    "    --input_model <path_to_model> \\\n",
    "    --output_dir <path_to_output_directory> \\\n",
    "    --data_type <data_precision> \\\n",
    "    --move_to_preprocess \\\n",
    "    --scale_values [<scale_values>] \\\n",
    "    --mean_values [<channel_mean_values>]\n",
    "```\n",
    "\n",
    "The input arguments are as follows:\n",
    "- **--input_model** : The model's input *.pb* file\n",
    "- **--output_dir** : Output directory where to store the generated IR model files\n",
    "- **--data_type** : The model's data type and precision (e.g. FP16, FP32, INT8, etc.)\n",
    "- **--move_to_preprocess** : Move mean values to IR preprocess section\n",
    "- **--scale_values** : Scaling (divide by, one per channel) value to apply to input values\n",
    "- **--mean_values** : Mean values (one per channel) to be subtracted from input values before scaling\n",
    "\n",
    "The input shape, scaling values, and mean values we will be using are specific to the model topology being used.  Using the appropriate values for the model that we will use, The complete command will look like the following:\n",
    "```bash\n",
    "!mo.py \\\n",
    "    --input_model <path_to_model> \\\n",
    "    --data_type <data_precision> \\\n",
    "    --output_dir models/<data_precision> \\\n",
    "    --move_to_preprocess \\\n",
    "    --scale_values [58.395,57.12,57.375] \\\n",
    "    --mean_values [123.75,116.28,103.58]\n",
    "```\n",
    "We will run the command once on each model with <*path_to_model*> set to `models/onnx/surgical_tools.onnx` and `models/onnx/surgical_tools_parts.onnx` and with <*data_precision*> set to `FP16`.  This may be changed (e.g. `FP32`, etc.) as needed to run inference on other different devices.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>More information on how to use Model Optimizer to convert ONNX* models as well as specifying input shape and scaling parameters for common model topologies may be found at:[Converting a ONNX* Model](https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html)\n",
    "</i></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to use the Model Optimizer to create the model IR files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mo_onnx.py \\\n",
    "    --input_model \"models/onnx/surgical_tools.onnx\" \\\n",
    "    --output_dir models/ov/FP16/ \\\n",
    "    --data_type FP16 \\\n",
    "    --move_to_preprocess \\\n",
    "    --scale_values \"[0.229, 0.224, 0.225]\" \\\n",
    "    --mean_values \"[0.485, 0.456, 0.406]\"\n",
    "\n",
    "!mo_onnx.py \\\n",
    "    --input_model \"models/onnx/surgical_tools_parts.onnx\" \\\n",
    "    --output_dir models/ov/FP16/ \\\n",
    "    --data_type FP16 \\\n",
    "    --move_to_preprocess \\\n",
    "    --scale_values \"[0.229, 0.224, 0.225]\" \\\n",
    "    --mean_values \"[0.485, 0.456, 0.406]\"\n",
    "\n",
    "!echo \"\\nAll IR files that were created:\"\n",
    "!find ./models -name \"*.xml\" -o -name \"*.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=tip><i><b>Tip: </b>The '!' at the beginning of a line is a special Jupyter* Notebook command that allows you to run shell commands as if you are at a command line. The above command will also work in a terminal (with the '!' removed).</i></div>\n",
    "\n",
    "As shown above from the output of the last `!find...` command, the required sets of IR model files (`*.xml` and `*.bin`) have been created.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "The following sections will go through the steps to run our inference application on the Intel® DevCloud for the Edge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure input\n",
    "For convenience and consistency, in the next cell we set the Python* variable `InputVideo` to the input video file we will be using to run our sample application.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>\n",
    "If you want to use a different input video, change the path in the following cell to the path of the video and run the cell again.\n",
    "</i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the input video to use for the rest of this sample\n",
    "InputVideo = \"data/short_source.mp4\"\n",
    "print(f\"Input video file set to:{InputVideo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the job file\n",
    "We will run inference on several different edge compute nodes present in the Intel® DevCloud for the Edge. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is a [Bash](https://www.gnu.org/software/bash/) script that serves as a wrapper around the Python* executable of our application that will be executed directly on the edge compute node.  One purpose of the job file is to simplify running an application on different compute nodes by accepting a few arguments and then performing accordingly any necessary steps before and after running the application executable.  \n",
    "\n",
    "For this sample, the job file we will be using is already written for you and appears in the next cell.  The job file will be submitted as if it were run from the command line using the following format:\n",
    "```bash\n",
    "segmentation_job.sh <output_directory> <device> <fp_precision> <input_file> \n",
    "```\n",
    "Where the job file input arguments are:\n",
    "- <*output_directory*> - Output directory to use to store output files\n",
    "- <*device*> - Hardware device to use (e.g. CPU, GPU, etc.)\n",
    "- <*fp_precision*> - Which floating point precision inference model to use (FP32 or FP16)\n",
    "- <*input_file*> - Path to input video file\n",
    "\n",
    "Based on the input arguments, the job file will do the following:\n",
    "- Change to the working directory `PBS_O_WORKDIR` where this Jupyter* Notebook and other files appear on the compute node\n",
    "- Create the <*output_directory*>\n",
    "- Choose the appropriate inference model IR file for the specified <*fp_precision*>\n",
    "- Run the application Python* executable with the appropriate command line arguments\n",
    "\n",
    "Run the following cell to create the `segmentation_job.sh` job file.  The [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) line at the top will write the cell contents to the specified job file `segmentation_job.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile segmentation_job.sh\n",
    "\n",
    "# Store input arguments: <output_directory> <device> <fp_precision> <input_file>\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "INPUT_FILE=$4\n",
    "\n",
    "# The default path for the job is the user's home directory,\n",
    "#  change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Make sure that the output directory exists.\n",
    "mkdir -p $OUTPUT_FILE\n",
    "\n",
    "# Run the robotic instrument segmentation code\n",
    "python3 python/segmentation_parts.py  -m $FP_MODEL \\\n",
    "                                      -i $INPUT_FILE \\\n",
    "                                      -d $DEVICE \\\n",
    "                                      -o $OUTPUT_FILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to submit a job\n",
    "\n",
    "Now that we have the job script, we can submit jobs to edge compute nodes in the Intel® DevCloud for the Edge.  To submit a job, the `qsub` command is used with the following format:\n",
    "```bash\n",
    "qsub <job_file> -N <JobName> -l <nodes> -F \"<job_file_arguments>\" \n",
    "```\n",
    "We can submit segmentation_job.sh to several different types of edge compute nodes simultaneously or just one node at a time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- <*job_file*> - This is the job file we created in the previous step\n",
    "- `-N` <*JobName*> : Sets name specific to the job so that it is easier to distinguish  between it and other jobs\n",
    "- `-l` <*nodes*> - Specifies the number and the type of nodes using the format *nodes*=<*node_count*>:<*property*>[:<*property*>...]\n",
    "- `-F` \"<*job_file_arguments*>\" - String containing the input arguments described in the previous step to use when running the job file\n",
    "\n",
    "*(Optional)*: To see the available types of nodes on the Intel® DevCloud for the Edge, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output from executing the previous cell, the properties describe the node, and the number on the left is the number of available nodes of that architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit jobs\n",
    "\n",
    "Each of the cells in the subsections below will submit a job to be run on different edge compute nodes. The output of each cell is the _JobID_ for the submitted job.  The _JobID_ can be used to track the status of the job.  After submission, a job will go into a waiting queue before running once the requested compute nodes become available.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>You may submit all jobs at once or one at a time.</i></div> \n",
    "<br><div class=note><i><b>Note: </b>Presently the model is not fully supported for VPU so we will not be running it on a Intel® Neural Compute Stick 2 nor a Intel® Vision Accelerator Design with Intel® Movidius™ VPUs.  There is an issue that prevents running on an Intel® Atom® and using the integrated Intel® GPU, so we will not be running on it too.</i></div>\n",
    "<br><div class=tip><b>Tip: </b>**Shift+Enter** will run the cell and automatically move you to the next cell. This allows you to use **Shift+Enter** multiple times to quickly run through multiple cells, including markdown cells.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with an Intel® CPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Core™ i5-6500TE](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz.html) processor. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub segmentation_job.sh -l nodes=1:idc001skl -F \"results/core/ CPU FP16 {InputVideo}\" -N robo_seg_core\n",
    "print(job_id_core[0])\n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/core',  f'i_progress_{job_id_core[0]}.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Xeon® Gold 6258R CPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Xeon® Gold 6258R Processor](https://ark.intel.com/content/www/us/en/ark/products/199350/intel-xeon-gold-6258r-processor-38-5m-cache-2-70-ghz.html). The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon_cascade_lake = !qsub segmentation_job.sh -l nodes=1:idc018 -F \"results/xeon_cascade_lake/ CPU FP16 {InputVideo}\" -N robo_seg_xeon_cascade_lake\n",
    "print(job_id_xeon_cascade_lake[0])\n",
    "#Progress indicators\n",
    "if job_id_xeon_cascade_lake:\n",
    "    progressIndicator('results/xeon_cascade_lake',  f'i_progress_{job_id_xeon_cascade_lake[0]}.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Xeon® E3-1268L v5 CPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Xeon® Processor E3-1268L v5](https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz.html). The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon_skylake = !qsub segmentation_job.sh  -l nodes=1:idc007xv5 -F \"results/xeon_skylake/ CPU FP16 {InputVideo}\" -N robo_seg_xeon_skylake \n",
    "print(job_id_xeon_skylake[0])\n",
    "#Progress indicators\n",
    "if job_id_xeon_skylake:\n",
    "    progressIndicator('results/xeon_skylake', f'i_progress_{job_id_xeon_skylake[0]}.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Submit to an edge compute node with Intel® Core CPU and using the integrated Intel® GPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Core i5-6500TE](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz.html). The inference workload will run on the Intel® HD Graphics 530 GPU integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub segmentation_job.sh -l nodes=1:idc001skl -F \"results/gpu/ GPU FP16 {InputVideo}\" -N robo_seg_gpu \n",
    "print(job_id_gpu[0])\n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/gpu', f'i_progress_{job_id_gpu[0]}.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate example visualizations using an Intel Core CPU\n",
    "In the cell below, we submit a job to run the script [figures.py](python/figures.py) on an edge node with an [Intel Core i5-6500TE](https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-). The workload will be run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script\n",
    "job_id_figures = !qsub python/run_py.sh -l nodes=1:idc001skl -F \"python/figures.py\" -N robo_seg_figures \n",
    "if job_id_figures:\n",
    "    print(job_id_figures[0])\n",
    "    progressIndicator('results/', job_id_figures[0]+'.txt', \"Visualization\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor job status\n",
    "\n",
    "To check the status of the jobs that have been submitted, use the `qstat` command.  The custom Jupyter* Notebook widget `liveQstat()` is provided to display the output of `qstat` with live updates.  \n",
    "\n",
    "Run the following cell to display the current job status with periodic updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs that you have submitted (referenced by the `JobID` that gets displayed right after you submit the jobs in the previous step).\n",
    "There should also be an extra job in the queue named `jupyterhub-singleuser`: this job is your current Jupyter* Notebook session which is always running.\n",
    "\n",
    "The `S` column shows the current status of each job: \n",
    "- If the status is `Q`, then the job is queued and waiting for available resources\n",
    "- If ste status is `R`, then the job is running\n",
    "- If the job is no longer listed, then the job has completed\n",
    "\n",
    "<br><div class=note><i><b>\n",
    "Note: The amount of time spent in the queue depends on the number of users accessing the requested compute nodes. Once the jobs for this sample application begin to run, they should take from 1 to 5 minutes each to complete.\n",
    "</b></i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=danger><b>Wait!: </b>Please wait for the inference jobs and video rendering to complete before proceeding to the next step to view results.</div>\n",
    "\n",
    "### View results\n",
    "\n",
    "Once the jobs have completed, the queue system outputs the stdout and stderr streams of each job into files with names of the forms <*JobName*>.o<*JobID*> and <*JobName*>.e<*JobID*>, respecitvely.  The *JobName* corresponds to the `-N` option when submitting the job using the `qsub` command.  \n",
    "\n",
    "The output video file for each job is written to the file `output.mp4` located in the directory `results/<device>` that was specified as the output directory to the job file.  We will now use the `videoHTML()` utility to display the output video files within this Jupyter* notebook.  Calling `videoHTML()` from a Python* cell follows the form:\n",
    "```python\n",
    "videoHTML(title, [list_of_video_files], statistics(optional))\n",
    "```\n",
    "The parameters are:\n",
    "- *title* - Title to put at the top of the displayed output\n",
    "- \\[*list_of_video_files*\\] - Python* list of video files to display\n",
    "- *statistics(optional)* - Optional statistics file containing the number of seconds it took to process a number of frames\n",
    "\n",
    "Run the cells below to display the videos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View results from an Intel® CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('Intel Core CPU', \n",
    "          [f'results/core/output_{job_id_core[0]}.mp4'],f'results/core/stats_{job_id_core[0]}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View results from an Intel® Xeon® Gold 6258R CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('Intel® Xeon® Gold 6258R CPU', \n",
    "          [f'results/xeon_cascade_lake/output_{job_id_xeon_cascade_lake[0]}.mp4'],f'results/xeon_cascade_lake/stats_{job_id_xeon_cascade_lake[0]}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View results from an Intel® Xeon® E3-1268L v5 CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('Intel® Xeon® E3-1268L v5 CPU',\n",
    "          [f'results/xeon_skylake/output_{job_id_xeon_skylake[0]}.mp4'],f'results/xeon_skylake/stats_{job_id_xeon_skylake[0]}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View results from an Intel® Core CPU and integrated Intel® GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('Intel Core + Intel GPU',\n",
    "          [f'results/gpu/output_{job_id_gpu[0]}.mp4'],f'results/gpu/stats_{job_id_gpu[0]}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize results on the  Intel® CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "from IPython.display import Image\n",
    "display(Image(filename='generated/predictions.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View and assess performance results\n",
    "\n",
    "The running time of each inference task is recorded in `results/<device>/stats.txt`. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance for **Inference Engine Processing Time** and higher values mean better performance for **Inference Engine FPS**. When comparing results, please keep in mind that some architectures are optimized for highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table of <architecture>, <title> for plotting\n",
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('xeon_cascade_lake', 'Intel Xeon\\nGold\\n 6258R\\nCPU'),\n",
    "             ('xeon_skylake', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nHD530/GPU')]\n",
    "# For each archtecture in table, create path to stats file or placeholder \n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    # if job_id_<architecture> exists, the job was run and has a stats file\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append((f'results/{arch}/stats_{vars()[\"job_id_\"+arch][0]}.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "# Plot the execution time from the stats files\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, seconds', 'Inference Engine Processing Time', 'time')\n",
    "# Plot the frames per second from the stats files\n",
    "summaryPlot(stats_list, 'Architecture', 'Frames per second', 'Inference Engine FPS', 'fps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telemetry Dashboard\n",
    "Once your submitted jobs are completed, run the cells below to view telemetry dashboards containing performance metrics for your model and target architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view telemetry dashboard of the last job ran on Intel® Core™ i5-6500TE</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + job_id_core[0].split('.')[0]\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view metering dashboard of the last job ran on Intel® Xeon® Gold 6258R CPU</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + job_id_xeon_cascade_lake[0].split('.')[0]\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view metering dashboard of the last job ran on Intel® Xeon® E3-1268L v5 CPU</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + job_id_xeon_skylake[0].split('.')[0]\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view telemetry dashboard of the last job ran on Intel® Core CPU and using the integrated Intel® GPU</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + job_id_gpu[0].split('.')[0]\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "Quantization is the process of reducing the model's precision. By performing this optimization, you can accelerate your model execution time.  \n",
    "\n",
    "[quantize.py](python/quantize.py) file contains quantization script and uses the [post training optimization toolkit (POT) API](https://docs.openvinotoolkit.org/latest/pot_compression_api_README.html) to reduce model's precision from FP32 to INT8. Quantization settings, such as the path to original model, path to dataset, quantization algorithm etc., which should be set via configs. ```DatasetsDataLoader``` creates quantization dataset from the sample video and loads one by one input images to POT, when quantization process starts. When quantization is finished, the INT8 model will be saved at ```'/models/ov/INT8'``` directory.\n",
    "\n",
    "Run the following cell to create the ```quantization_job.sh``` job file. This script runs quantization and benchmarking of the quantized and non-quantized models to compare their execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile quantization_job.sh\n",
    "export WORK_DIR=$PBS_O_WORKDIR\n",
    "\n",
    "# Run the quantization script\n",
    "python3 ${WORK_DIR}/python/quantize.py\n",
    "\n",
    "# Run the benchmark_app for FP32 model\n",
    "python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py \\\n",
    "        -m ${WORK_DIR}/models/ov/FP16/surgical_tools_parts.xml 2>/dev/null | grep Throughput | xargs echo FP32\n",
    "\n",
    "# Run the benchmark_app for INT8 model\n",
    "python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py \\\n",
    "        -m ${WORK_DIR}/models/ov/INT8/surgical_tools_parts.xml 2>/dev/null | grep Throughput | xargs echo INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to start quantization and benchmarking of the quantized model on Xeon Gold 6258R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job submitted to the queue. Waiting for it to complete: 68266.v-qsvr-1.devcloud-edge\n",
      "Results for non-optimized and optimized models:\n",
      "FP32 Throughput: 2.57 FPS\n",
      "INT8 Throughput: 6.07 FPS\n"
     ]
    }
   ],
   "source": [
    "def wait_for_job_to_finish(job_id):\n",
    "    if job_id:\n",
    "        print(\"Job submitted to the queue. Waiting for it to complete: \" + job_id[0])    \n",
    "        \n",
    "        filename = os.getcwd() + \"/quantization_job.{}{}\"\n",
    "        \n",
    "        filename_o = filename.format('o',job_id[0].split(\".\")[0])\n",
    "        filename_e = filename.format('e',job_id[0].split(\".\")[0])\n",
    "            \n",
    "        # Wait until the job is ended and the report file is created.\n",
    "        while not os.path.exists(filename_o) and not os.path.exists(filename_e):  \n",
    "            time.sleep(1) \n",
    "            \n",
    "        # Reading the benchmark_app results\n",
    "        print('Results for non-optimized and optimized models:')\n",
    "        with open(filename_o, 'r') as read_obj:\n",
    "            for line in read_obj:            \n",
    "                if 'Throughput:' in line:\n",
    "                    print(line.split('\\n')[0])\n",
    "                      \n",
    "    else:\n",
    "        print(\"Error in job submission.\")\n",
    "\n",
    "job_id_core = get_ipython().getoutput('qsub quantization_job.sh -N quantization_job -l nodes=1:gold6258r')\n",
    "benchmarks = wait_for_job_to_finish(job_id_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=danger><b>Wait! </b>Please wait for the inference quantization job to complete.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo only 20 images were used for the model's quantization from FP32 to INT8. \n",
    "To achieve better accuracy, more data should be processed in quantization process to collect more robust statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- [More Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials) - sample application Jupyter* Notebook tutorials\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "For technical support, please see the [Intel® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "    @inproceedings{shvets2018automatic,\n",
    "    title={Automatic Instrument Segmentation in Robot-Assisted Surgery using Deep Learning},\n",
    "    author={Shvets, Alexey A and Rakhlin, Alexander and Kalinin, Alexandr A and Iglovikov, Vladimir I},\n",
    "    booktitle={2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)},\n",
    "    pages={624--628},\n",
    "    year={2018}\n",
    "    }\n",
    "<br><div class=note><i><b>Note: </b>Citation appears exactly as it appears on source website</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2021.1)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "685.455px",
    "left": "232px",
    "top": "601.284px",
    "width": "251.392px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
