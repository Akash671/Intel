{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO version check:\n",
    "You are currently using the latest development version of Intel® Distribution of OpenVINO™ Toolkit. Alternatively, you can open a version of this notebook for the Intel® Distribution of OpenVINO™ Toolkit LTS version by [clicking this link](../../../../openvino-lts/developer-samples/cpp/speech_recognition/speech_recognition.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Speech Recognition\n",
    "\n",
    "This is a sample reference implementation to showcase an offline speech recognition application using an audio file, and a Kaldi acoustic model, a kaldi language model. In addition to Highlighting the fundamental Libraries:  \n",
    "* OpenVINO™ Inference Engine\n",
    "* Intel® Speech Feature Extraction (Speech Library)\n",
    "* Intel® Speech Decoder libraries (Speech Library)\n",
    "\n",
    "<img src=\"img/speech_recognition_pipeline.png\">\n",
    "\n",
    "\n",
    "\n",
    "## Overview of how it works\n",
    "At start-up the sample application reads command line arguments and loads the audio file \n",
    "(.wav) in memory and parses the configuration file that indicates the inference device, the path to model IR(.xml+.bin), and other specific configurations pertaining to the Intel® Speech Feature Extraction and Intel® Speech Decoder libraries described in the configuration file section where they'll be initialized. After Initialization, the audio file will go through feature extraction, feature vectors will then serve as input to the acoustic model where inference will be ran to transcribe them to context-dependent phonemes, and lastly using a language model to decode the phonemes to words.\n",
    "\n",
    "A job is submitted to a hardware accelerator Intel® Gaussian & Neural Accelerator(GNA), Intel® Core U-series (Whiskey-Lake) CPU, Intel® Core CPU,Intel® Xeon® CPU,  Intel® HD Graphics GPU, and Intel® Xeon).\n",
    "After the processing of the entire audio is completed, the output results are appropriately stored in the <JOB_NAME>.o<JOB_ID> file in the current working directory, which can then be viewed within the Jupyter Notebook instance.\n",
    "\n",
    "## Demonstration objectives\n",
    "* Audio input with an acoustic model for Inference\n",
    "* Inference performed on edge hardware (rather than on the development node hosting this Jupyter notebook)\n",
    "* Demonstrate the Speech Library API in action\n",
    "\n",
    "\n",
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import dependencies\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results and listening to the audio in this notebook\n",
    "(tip: select the cell and use **Ctrl+enter** to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Audio\n",
    "import os\n",
    "import time\n",
    "import sys                                     \n",
    "from qarpo.demoutils import *\n",
    "\n",
    "#install pyyaml for parsing config\n",
    "!pip3 install --user -U PyYAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2: Build the speech library, the kaldi slm tool, and the offline speech recognition app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./build_speech_lib.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the processing of on an audio file to see how the Speech Library and OpenVINO's Inference Engine work to extract features, run inference on features, and decode the phonemes to text.\n",
    "\n",
    "We will go over the Speech Recognition Pipeline with OpenVINO in several steps:\n",
    "\n",
    "1. Create a configuration file.\n",
    "2. Understand the Speech library API that includes the Intel® Speech Feature Extraction and Intel® Speech Decoder.\n",
    "3. Execute the Offline Speech Recognition application on Development Node(CPU).\n",
    "6. Create a job file to target different hardware types.\n",
    "7. Submit jobs to the queue.\n",
    "8. View the results and hardware performance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the pre-trained LibreSpeech DNN model created from the Kaldi S5 NNet1 framework. \n",
    "**We do not have to use model optimizer that is responsible for converting a model to IR (.xml+bin) since that step has already been done and we are downloading the IR files.**\n",
    "\n",
    "The script will produce the following files:\n",
    "* **speech_recognition_config.template** - This is a template for the configuration of the parameters/options to set for the feature extraction step, the inference step, and the decode step. \n",
    "* **lspeech_s5_ext.feature_transform** - The feature transform file holds a fixed function that serves as a front end and expands dimensionality so that low dimension inputs can be used thus saving disk-space and read throughput. It's used during feature extraction stage.\n",
    "* **lspeech_s5_ext.xml** - IR file of the acoustic model to understand the layers, each layer's parameters, and how the model is connected. (human-readable) \n",
    "* **lspeech_s5_ext.bin** -The IR file holds the weights of the acoustic model. \n",
    "* **hclg.fst** - This is the language model/decoding graph (Finite State Transducer (.fst)) that is based on the transducer (h), phonetic context (c), Lexicon (l), and grammar (g). [Visit Kaldi Documentation to read more about this](https://kaldi-asr.org/doc/graph_recipe_test.html).\n",
    "* **labels.bin**  - This holds the symbol table responsible for describing the alphabet of the input and output labels for arcs in the Finite State Transducer (hclg.fst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 speech_recognition_model.py -c /opt/intel/openvino/data_processing/audio/speech_recognition/models/intel/lspeech_s5_ext/model.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is critical into setting the parameters for each specific stage: Feature Extraction, Inference Engine, and the Decoder. We will highlight the **critical parameters** changed for each stage in the pipeline for this current sample. To understand more about all the options offered in each stage that is configurable visit the [OpenVINO documentation](https://docs.openvinotoolkit.org/latest/_inference_engine_samples_speech_libs_and_demos_Offline_speech_recognition_demo.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three configuration files that are pre-configured for you to use:\n",
    "* speech_lib_CPU.cfg\n",
    "* speech_lib_GPU.cfg\n",
    "* speech_lib_GNA.cfg\n",
    "\n",
    "We will show the CPU specific configuration file for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat speech_lib_CPU.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-fe:rt:featureTransform** -> path to kaldi feature transform file \n",
    "\n",
    "Example:\n",
    "\n",
    "**-fe:rt:featureTransform** model/FP32/lspeech_s5_ext.feature_transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Engine Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-inference:device** -> The device used to run inference (CPU|GPU|GNA_AUTO)\n",
    "\n",
    "Example:\n",
    "\n",
    "**-inference:device** CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-dec:wfst:acousticModelFName** - path to the acoustic model .xml file without .xml extension\n",
    "\n",
    "Example: \n",
    "\n",
    "**-dec:wfst:acousticModelFName** model/FP32/lspeech_s5_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-dec:wfst:fsmFName** - path to language model \n",
    "\n",
    "Example:\n",
    "\n",
    "**-dec:wfst:fsmFName** model/FP32/hclg.fst "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-dec:wfst:outSymsFName** - Path to Symbols file.\n",
    "\n",
    "Example:\n",
    "\n",
    "**-dec:wfst:outSymsFName** model/FP32/labels.bin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(\"/data/reference-sample-data/speech-recognition/how_are_you_doing.wav\",autoplay=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Speech Library serves as a wrapper around the Intel® Speech Feature Extraction and Intel® Speech Decoder libraries that takes care of the initialization of core components and data passing while exposing a simple C++ API.\n",
    "\n",
    "5 Main Function Calls:\n",
    "\n",
    "* **SpeechLibraryCreate** - Creates an instance of Speech Library with a callback Handle. \n",
    "* **SpeechLibraryInitialize** - Takes the Speech Library Handle and Configuration file, Parses the configuration file to load and  initialize the proper settings(wav file, acousting model, language file, etc.) in the configuration for each stage to build the pipeline (Feature Extraction, Inference, and Decode).\n",
    "* **SpeechLibraryPushData** - Takes the audio data either from wav file or mic pushing it through the entire pipeline starting with the feature extractor -> Inference Engine-> Decoder.\n",
    "* **SpeechLibraryGetResult** - Returns the transcribed audio. \n",
    "* **SpeechLibraryRelease** - Releases all resources tied to the handle for current speech recognition pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech API Call Flow Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/speech_library_api.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech API Used In Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n 227,265p /opt/intel/openvino/data_processing/audio/speech_recognition/demos/offline_speech_recognition_demo/src/speech_library_app.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on Dev Node CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./intel64/Release/offline_speech_recognition_app -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./intel64/Release/offline_speech_recognition_app -wave=/data/reference-sample-data/speech-recognition/how_are_you_doing.wav -c=speech_lib_CPU.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Job File\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel® Xeon® Scalable Processor, where the Notebook is allocated a single core. We will run the workload on several DevCloud's edge compute nodes. We will send work to the edge compute nodes by submitting jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass the specific variables to the C++ code, we will use following arguments:\n",
    "\n",
    "* `-c`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;location of the configuration file\n",
    "* `-i`&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;location of the wav file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job file will be executed directly on the edge compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile speech_recognition_job.sh\n",
    "\n",
    "ME=`basename $0`\n",
    "\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "while getopts 'c:i:?' OPTION; do\n",
    "    case \"$OPTION\" in\n",
    "\n",
    "    c)\n",
    "        CONFIG_FILE=$OPTARG\n",
    "        echo \"$ME is using config file $OPTARG\"\n",
    "      ;;\n",
    "\n",
    "    i)\n",
    "        WAVE_FILE=$OPTARG\n",
    "        echo \"$ME is using wave file $OPTARG\"\n",
    "      ;;\n",
    "    esac  \n",
    "done\n",
    "\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/intel/intelpython3/lib/\n",
    "\n",
    "./intel64/Release/offline_speech_recognition_app -wave=$WAVE_FILE -c=$CONFIG_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Job queue submission\n",
    "\n",
    "Each cell below will submit a job to different edge compute nodes.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all 5 jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells)\n",
    "\n",
    "**Note** If you want to use your own video, Change the environment variable 'VIDEO' in the following cell from \"/data/reference-sample-data/safety-gear-detection/Safety_Full_Hat_and_Vest.mp4\" to the full path of your uploaded video.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AUDIO\"] = \"/data/reference-sample-data/speech-recognition/how_are_you_doing.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10th Generation  Intel® Core CPU with GNA\n",
    "In the cell below, we submit a job to an edge node with a <a href=\"https://www.intel.com/content/www/us/en/products/processors/core/i7-processors/i7-1065g7.html\">Intel 10th Generation Intel Core CPU</a>. The inference workload will run on the GNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gna = !qsub speech_recognition_job.sh -l nodes=qsub -l nodes=1:i7-1065g7 -F \"-c speech_lib_GNA.cfg -i $AUDIO \" -N speech_gna\n",
    "print(job_id_gna[0]) \n",
    "#For viewing results\n",
    "output_file_gna = \"speech_gna.o\"+job_id_gna[0].split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8th Generation  Intel® Core CPU \n",
    "In the cell below, we submit a job to an edge node with an <a href=\"https://www.intel.com/content/www/us/en/design/products-and-solutions/processors-and-chipsets/whiskey-lake/overview.html\">Intel 8th Generation Intel Core Whiskey Lake CPU</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_whiskeylake = !qsub speech_recognition_job.sh -l nodes=1:idc016ai7 -F \"-c speech_lib_CPU.cfg -i $AUDIO \" -N speech_whiskeylake_cpu\n",
    "print(job_id_whiskeylake[0]) \n",
    "#For viewing results\n",
    "output_file_whiskeylake_cpu = \"speech_whiskeylake_cpu.o\"+job_id_whiskeylake[0].split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® CPU \n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel \n",
    "    Core i5-6500TE</a>. The inference workload will run on the CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub speech_recognition_job.sh -l nodes=1:idc001skl -F \"-c speech_lib_CPU.cfg -i $AUDIO \" -N speech_core\n",
    "print(job_id_core[0]) \n",
    "#For viewing results\n",
    "output_file_core = \"speech_core.o\"+job_id_core[0].split('.')[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® Xeon® CPU \n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel \n",
    "    Xeon Processor E3-1268L v5</a>. The inference workload will run on the CPU.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub speech_recognition_job.sh -l nodes=1:idc007xv5 -F \"-c speech_lib_CPU.cfg -i $AUDIO \" -N speech_xeon\n",
    "print(job_id_xeon[0]) \n",
    "#For viewing results\n",
    "output_file_xeon = \"speech_xeon.o\"+job_id_xeon[0].split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® Core CPU with Intel® GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub speech_recognition_job.sh -l nodes=1:idc001skl -F \"-c speech_lib_GPU.cfg -i $AUDIO \" -N speech_gpu\n",
    "print(job_id_gpu[0]) \n",
    "#For viewing results\n",
    "output_file_gpu = \"speech_gpu.o\"+job_id_gpu[0].split('.')[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UP Squared Grove IoT Development Kit\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/up-squared-grove-dev-kit\">UP Squared Grove IoT Development Kit</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/96488/Intel-Atom-x7-E3950-Processor-2M-Cache-up-to-2-00-GHz-\">Intel Atom® x7-E3950 Processor</a>. The inference  workload will run on the integrated Intel® HD Graphics 505 card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_up2 = !qsub speech_recognition_job.sh -l nodes=1:idc008u2g -F \"-c speech_lib_GPU.cfg -i $AUDIO \" -N speech_up2_gpu\n",
    "print(job_id_up2[0]) \n",
    "#For viewing results\n",
    "output_file_up2_gpu = \"speech_up2_gpu.o\"+job_id_up2[0].split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Progress\n",
    "\n",
    "Check the progress of the jobs. `Q` status stands for `queued`, `R` for `running`. How long a job is being queued is dependent on number of the users. It should take up to 5 minutes for a job to run. If the job is no longer listed, it's done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs and video rendering complete before proceeding to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the `stdout` and `stderr` streams of each job into files with names\n",
    "`speech_{type}.o{JobID}` and `obj_det_{type}.e{JobID}`. Here, speech_{type} corresponds to the `-N` option of qsub. For example, `core` for Core CPU target.\n",
    "\n",
    "\n",
    "`speech_{type}.e{JobID}`\n",
    "\n",
    "(here, speech_{type} corresponds to the `-N` option of qsub).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10th Generation  Intel® Core CPU with GNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd()+\"/\"+output_file_gna\n",
    "fd = open( filepath, 'r')\n",
    "print(fd.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8th Generation  Intel® Core CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd()+\"/\"+output_file_whiskeylake_cpu\n",
    "fd = open( filepath, 'r')\n",
    "print(fd.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd()+\"/\"+output_file_core\n",
    "fd = open( filepath, 'r')\n",
    "print(fd.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Intel® Xeon® CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd()+\"/\"+output_file_xeon\n",
    "fd = open( filepath, 'r')\n",
    "print(fd.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel® Core CPU with Intel® GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd()+\"/\"+output_file_gpu\n",
    "fd = open( filepath, 'r')\n",
    "print(fd.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UP Squared Grove IoT Development Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.getcwd()+\"/\"+output_file_up2_gpu\n",
    "fd = open( filepath, 'r')\n",
    "print(fd.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- [More Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials) - sample application Jupyter* Notebook tutorials\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "For technical support, please see the [Intel® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> |\n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2021.1)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "182px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
